{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3249b069-318c-46cf-a0ab-422a39d81733",
   "metadata": {},
   "source": [
    "# **Template OP on salish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b3cacf0-e987-4dcc-85ad-fa33f293ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from parcels import FieldSet, Field, VectorField, ParticleSet, JITParticle, ErrorCode, ParcelsRandom\n",
    "\n",
    "sys.path.append('/home/jvalenti/MOAD/analysis-jose/notebooks/parcels')\n",
    "from Kernels_beaching import DeleteParticle, Buoyancy, AdvectionRK4_3D, Stokes_drift, Beaching\n",
    "from OP_functions_beaching import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e0ace",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "941b8dd3-eebb-4149-af84-781e66652a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "local = 0 #Set to 0 when working on server\n",
    "paths = path(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206d74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dat=xr.open_dataset(get_WW3_path(datetime(2018, 12, 23)))\n",
    "path_NEMO = make_prefix(datetime(2018, 12, 23), paths['NEMO'])\n",
    "Dat0=xr.open_dataset(path_NEMO + '_grid_W.nc')\n",
    "Dat=xr.open_dataset(path_NEMO + '_grid_T.nc')\n",
    "coord=xr.open_dataset(paths['coords'],decode_times=False)\n",
    "WW3 = xr.open_dataset(get_WW3_path(datetime(2018, 12, 23)))\n",
    "batt=xr.open_dataset(paths['mask'],decode_times=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cd801",
   "metadata": {},
   "source": [
    "## Define and save mask for distance to coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "518b09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def maskcoast(ilat,jlon):\n",
    "#     baty = batt.mbathy[0,:,:]\n",
    "#     maski = 0\n",
    "#     if baty[ilat,jlon]>1e-1:\n",
    "#         if baty[ilat+1,jlon] == 0 or baty[ilat+1,jlon+1] == 0 or baty[ilat+1,jlon-1] == 0:\n",
    "#             maski = 1\n",
    "#         elif baty[ilat,jlon] == 0 or baty[ilat,jlon+1] == 0 or baty[ilat,jlon-1] == 0:\n",
    "#             maski = 1\n",
    "#         elif baty[ilat-1,jlon] == 0 or baty[ilat-1,jlon+1] == 0 or baty[ilat-1,jlon-1] == 0:\n",
    "#             maski = 1\n",
    "#     return maski\n",
    "# maskd = np.zeros(batt.mbathy.shape)\n",
    "# baty = batt.mbathy[0,:,:]\n",
    "# for i in range(baty.shape[0]):\n",
    "#     for j in range(baty.shape[1]):\n",
    "#         maskd[0,i,j]=maskcoast(i,j)\n",
    "# batt=xr.open_dataset(paths['mask'],decode_times=False)\n",
    "# dist=xr.DataArray(attrs={'d':maskd})\n",
    "# batt['d']=(batt.mbathy.dims,maskd)\n",
    "# batt.to_netcdf(path='/ocean/jvalenti/MOAD/grid/mesh_maskd201702.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10b6fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clat,clon = p_unidist(coord.gphif[0,:,:],coord.glamf[0,:,:],batt.mbathy[0,:,:],10,10)\n",
    "with open('clat.txt') as f:\n",
    "    clat = f.read()\n",
    "    clat= clat[1:-1]\n",
    "    clat0 = clat.split(\",\")\n",
    "    f.close()\n",
    "with open('clon.txt') as f:\n",
    "    clon = f.read()\n",
    "    clon=clon[1:-1]\n",
    "    clon0 = clon.split(\",\")\n",
    "    f.close()\n",
    "    \n",
    "clat,clon=[],[]\n",
    "for i in range(len(clat0)):\n",
    "    clat.append(float(clat0[i]))\n",
    "    clon.append(float(clon0[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4bf2f-db05-4d5f-b4bb-bdaad6578599",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28a99025-a33d-4d66-9e39-aab453f279b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2018, 10, 1) #Start date\n",
    "# Set Time length [days] and timestep [seconds]\n",
    "length = 10 \n",
    "duration = timedelta(days=length)\n",
    "dt = 90 #toggle between - or + to pick backwards or forwards\n",
    "N = len(clat) # number of deploying locations\n",
    "n = 1 # 1000   # number of particles per location\n",
    "dmin = list(np.zeros(len(clat))) #minimum depth\n",
    "dd = 5 #max depth difference from dmin\n",
    "x_offset, y_offset, zvals = p_deploy(N,n,dmin,dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c87107ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parcels import Variable\n",
    "\n",
    "class MPParticle(JITParticle):        \n",
    "    ro = Variable('ro', initial = 1025)           \n",
    "    diameter = Variable('diameter', initial = 1.6e-5)\n",
    "    length = Variable('length', initial = 61e-5)\n",
    "    Lb = Variable('Lb', initial = 0.0014)  #times needed in days for particle to have 67% probability of beaching if in beaching zone (500m)\n",
    "    sediment = Variable('sediment', initial = 0)\n",
    "    beached = Variable('beached', initial = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ceb066c-22eb-4ef5-abe1-dc051ad4cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = np.zeros([N,n])\n",
    "lat = np.zeros([N,n])\n",
    "for i in range(N):\n",
    "    lon[i,:]=(clon[i] + x_offset[i,:])\n",
    "    lat[i,:]=(clat[i] + y_offset[i,:])\n",
    "z = zvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92d6511f-3d0c-4a98-ad32-0d4cb970a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jvalenti/MOAD/results/beachingL220181001_1n_20181011_1n.nc\n"
     ]
    }
   ],
   "source": [
    "#Set start date time and the name of the output file\n",
    "name = 'beachingL2' #name output file\n",
    "daterange = [start+timedelta(days=i) for i in range(length)]\n",
    "fn =  name + '_'.join(d.strftime('%Y%m%d')+'_1n' for d in [start, start+duration]) + '.nc'\n",
    "outfile = os.path.join(paths['out'], fn)\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e63f2-3fb8-4526-b45a-ce5644646287",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "922eb3dc-e34d-4102-b1d4-4df9de8b2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File /results2/SalishSea/nowcast-green.201905/07oct18/SalishSea_1h_20181007_20181007_carp_T.nc could not be decoded properly by xarray (version 0.19.0).\n",
      "         It will be opened with no decoding. Filling values might be wrongly parsed.\n"
     ]
    }
   ],
   "source": [
    "#Fill in the list of variables that you want to use as fields\n",
    "varlist=['U','V','W','R']\n",
    "filenames,variables,dimensions=filename_set(start,length,varlist,local)\n",
    "field_set=FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "\n",
    "varlist=['US','VS','WL']\n",
    "filenames,variables,dimensions=filename_set(start,length,varlist,local)\n",
    "\n",
    "us = Field.from_netcdf(filenames['US'], variables['US'], dimensions,allow_time_extrapolation=True)\n",
    "vs = Field.from_netcdf(filenames['VS'], variables['VS'], dimensions,allow_time_extrapolation=True)\n",
    "wl = Field.from_netcdf(filenames['WL'], variables['WL'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(us)\n",
    "field_set.add_field(vs)\n",
    "field_set.add_field(wl)\n",
    "field_set.add_vector_field(VectorField(\"stokes\", us, vs, wl))\n",
    "\n",
    "filenames,variables,dimensions=filename_set(start,length,['Bathy'],local)\n",
    "Bth = Field.from_netcdf(filenames['Bathy'], variables['Bathy'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Bth)\n",
    "\n",
    "filenames,variables,dimensions=filename_set(start,length,['D'],local)\n",
    "Distc = Field.from_netcdf(filenames['D'], variables['D'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Distc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f87621-a497-4575-85ba-18ec93118e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled ArrayMPParticleAdvectionRK4_3DBuoyancyStokes_driftBeaching ==> /tmp/parcels-2894/libba252d6ae0aac11548534427ddcba988_0.so\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle 1670 lost !! [5400.0, 2.249155692755852, 50.448851484720734, -126.01542965405797]\n",
      "Particle 1678 lost !! [5400.0, 1.1750005756469706, 50.460732275479245, -126.10717198850483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Temporary output files are stored in /home/jvalenti/MOAD/results/out-JETBFWBP.\n",
      "INFO: You can use \"parcels_convert_npydir_to_netcdf /home/jvalenti/MOAD/results/out-JETBFWBP\" to convert these to a NetCDF file during the run.\n",
      "  1% (12600.0 of 864000.0) |             | Elapsed Time: 0:00:09 ETA:   0:16:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle 1671 lost !! [19170.0, 0.14614771491047565, 50.45885432454082, -125.99977427862729]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (16200.0 of 864000.0) |             | Elapsed Time: 0:00:11 ETA:   0:09:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle 1668 lost !! [23310.0, 33.75511754937194, 50.43822200217546, -125.99962234459527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% (72000.0 of 864000.0) |#            | Elapsed Time: 0:00:44 ETA:   0:08:15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle 1672 lost !! [77760.0, 4.511737773854587, 50.45425997254297, -125.99984182487364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (864000.0 of 864000.0) |############| Elapsed Time: 0:09:02 Time:  0:09:02\n",
      "/home/jvalenti/conda_envs/parcels/lib/python3.9/site-packages/numpy/lib/arraysetops.py:270: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    }
   ],
   "source": [
    "# #Load Salish output as fields\n",
    "#field_set = FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z, time=start+timedelta(hours=2))\n",
    "\n",
    "k_sink = pset.Kernel(Buoyancy)\n",
    "k_waves = pset.Kernel(Stokes_drift)\n",
    "k_beach = pset.Kernel(Beaching)\n",
    "\n",
    "pset.execute(AdvectionRK4_3D + k_sink + k_waves + k_beach,\n",
    "             runtime=duration, \n",
    "             dt=dt,\n",
    "             output_file=pset.ParticleFile(name=outfile, outputdt=timedelta(hours=1)),\n",
    "             recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
