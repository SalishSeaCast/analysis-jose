{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from parcels import FieldSet, Field, VectorField, ParticleSet, JITParticle, ErrorCode, ParcelsRandom, Variable\n",
    "\n",
    "sys.path.append('/users/jvalenti/MOAD/analysis-jose/Source')\n",
    "\n",
    "from OP_functions23 import *\n",
    "from OP_Kernels23 import *\n",
    "\n",
    "local = 1\n",
    "restart=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(start,length):\n",
    "    timestamps=[]\n",
    "    duration = timedelta(days=length)\n",
    "    for day in range(duration.days):\n",
    "        timestamps.append([start + timedelta(days=day)])\n",
    "    return np.array(timestamps, dtype='datetime64')\n",
    "\n",
    "def find_temp(rootdir):\n",
    "    dirs=[]\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        if os.path.isdir(d):\n",
    "            dirs.append(d)\n",
    "    temp=sorted(dirs, key=lambda x: os.path.getctime(x), reverse=True)[:1][0]\n",
    "    return temp[-12:]\n",
    "\n",
    "def newest(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "paths = path(local)\n",
    "#Load configuration from .yaml\n",
    "param = load_config(['/users/jvalenti/MOAD/analysis-jose/OParcels/yaml/test.yaml'])\n",
    "#Definitions\n",
    "start = datetime(param['startdate']['year'], param['startdate']['month'], param['startdate']['day']) #Start date\n",
    "length = param['param']['length'] # Set Time length [days] \n",
    "dt = param['param']['dt'] #toggle between - or + to pick backwards or forwards \n",
    "N0 = param['param']['N'] # number of deploying locations\n",
    "n = param['param']['n'] # 1000   # number of particles per location\n",
    "dmin = param['param']['dmin'] #minimum depth\n",
    "dd = param['param']['dd'] #max depth difference from dmin\n",
    "name = param['file']['name'] #name output file\n",
    "dtp = param['param']['dtp'] #how often particle released in hours\n",
    "odt = param['param']['odt'] #how often data is recorded\n",
    "rrr = param['param']['r'] #radious of particle deployment\n",
    "distr = param['param']['distr']\n",
    "MFc = param['param']['MFc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set deploy coordinates following yaml   \n",
    "\n",
    "clat = param['param']['lats']\n",
    "clon = param['param']['lons']\n",
    "#clon, clat = [float(outf_lon)],[float(outf_lat)] \n",
    "duration = timedelta(days=length)\n",
    "#Set deploy locations\n",
    "if distr == 'hmg':\n",
    "    clat,clon = p_unidist(N0,N0)\n",
    "    N = len(clat)\n",
    "elif distr == 'trst':\n",
    "    clat,clon = transect_deploy(clat,clon,N0)\n",
    "    N = N0\n",
    "elif distr == 'std':\n",
    "    N = len(param['param']['lats'])\n",
    "elif distr == 'pd':\n",
    "    clat, clon, N = pandas_deploy(N0,MFc,int(dtp))\n",
    "    n = 1\n",
    "\n",
    "x_offset, y_offset, z = p_deploy(N,n,dmin,dd,rrr)\n",
    "\n",
    "lon = np.zeros([N,n])\n",
    "lat = np.zeros([N,n])\n",
    "for i in range(N):\n",
    "    lon[i,:]=(clon[i] + x_offset[i,:])\n",
    "    lat[i,:]=(clat[i] + y_offset[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set start date time and the name of the output file\n",
    "\n",
    "daterange = [start+timedelta(days=i) for i in range(length)]\n",
    "fn =  name + '_'.join(d.strftime('%Y%m%d')+'_1n' for d in [start, start+duration]) + '.nc'\n",
    "outfile = os.path.join(paths['out'], fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File /ocean/jvalenti/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2022.6.0).\n",
      "         It will be opened with no decoding. Filling values might be wrongly parsed.\n"
     ]
    }
   ],
   "source": [
    "####BUILD FIELDS FOR SIMULATION######\n",
    "#Fill in the list of variables that you want to use as fields\n",
    "varlist=['U','V','W']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "dimensions = {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw','time': 'time_counter'}\n",
    "field_set=FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "\n",
    "#Find file names and variable names\n",
    "varlist=['US','VS','WL','Diat','Flag','R','T','S','FS','ssh','Bathy','Kz']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "\n",
    "#Add Stokes Drift fields\n",
    "dimensions = {'lon': 'longitude', 'lat': 'latitude', 'time': 'time'}\n",
    "us = Field.from_netcdf(filenames['US'], variables['US'], dimensions,allow_time_extrapolation=True)\n",
    "vs = Field.from_netcdf(filenames['VS'], variables['VS'], dimensions,allow_time_extrapolation=True)\n",
    "wl = Field.from_netcdf(filenames['WL'], variables['WL'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(us)\n",
    "field_set.add_field(vs)\n",
    "field_set.add_field(wl)\n",
    "field_set.add_vector_field(VectorField(\"stokes\", us, vs, wl))\n",
    "\n",
    "#Add Vertical diffusivity coefficient field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'depthw','time': 'time_counter'}\n",
    "Kz = Field.from_netcdf(filenames['Kz'], variables['Kz'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Kz)\n",
    "\n",
    "#Add fields located at node T\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'deptht','time': 'time_counter'}\n",
    "Diat = Field.from_netcdf(filenames['Diat'], variables['Diat'], dimensions,allow_time_extrapolation=True)\n",
    "Flag = Field.from_netcdf(filenames['Flag'], variables['Flag'], dimensions,allow_time_extrapolation=True)\n",
    "R = Field.from_netcdf(filenames['R'], variables['R'], dimensions,allow_time_extrapolation=True)\n",
    "S = Field.from_netcdf(filenames['S'], variables['S'], dimensions,allow_time_extrapolation=True)\n",
    "T = Field.from_netcdf(filenames['T'], variables['T'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Diat)\n",
    "field_set.add_field(Flag)\n",
    "field_set.add_field(R)\n",
    "field_set.add_field(S)\n",
    "field_set.add_field(T)\n",
    "\n",
    "#Add Bathymetry 2D field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit'}\n",
    "Bth = Field.from_netcdf(filenames['Bathy'], variables['Bathy'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Bth)\n",
    "\n",
    "#Add SSH and Rivers 2D fields\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit','time': 'time_counter'}\n",
    "cms = Field.from_netcdf(filenames['ssh'], variables['ssh'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(cms)\n",
    "Fraser = Field.from_netcdf(filenames['FS'], variables['FS'], dimensions,allow_time_extrapolation=True,timestamps=get_timestamps(start,length))\n",
    "field_set.add_field(Fraser)\n",
    "\n",
    "####BUILD Particle typeN######\n",
    "MPParticle = particle_maker(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart==1:\n",
    "    name_temp=find_temp(paths['out'])\n",
    "    os.system(f\"cd {paths['out']} && parcels_convert_npydir_to_netcdf {name_temp}\")\n",
    "    outfile=newest(paths['out'])\n",
    "    pset = ParticleSet.from_particlefile(field_set, MPParticle,outfile)\n",
    "else:\n",
    "    if dtp == 0:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z,time=start+timedelta(hours=odt))\n",
    "    else:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z, repeatdt = timedelta(hours=dtp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_asem(pset,config):\n",
    "    KER = AdvectionRK4_3D \n",
    "    if 'Buoyancy' in config['kernel']:\n",
    "        KER += pset.Kernel(Buoyancy)\n",
    "    if 'Stokes_drift' in config['kernel']:\n",
    "        KER += pset.Kernel(Stokes_drift)\n",
    "    if 'Beaching' in config['kernel']:\n",
    "        KER += pset.Kernel(Beaching)\n",
    "        KER += pset.Kernel(Unbeaching)\n",
    "    if 'Turb_mix' in config['kernel']:\n",
    "        KER += pset.Kernel(turb_mix)\n",
    "    if 'Biofilm' in config['kernel']:\n",
    "        KER += pset.Kernel(Biofilm)\n",
    "    if 'Stokes_driftRK4_3D' in config['kernel']:\n",
    "        KER += pset.Kernel(Stokes_driftRK4_3D)\n",
    "    \n",
    "    return KER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled ArrayMPParticleAdvectionRK4_3DStokes_driftBeachingUnbeachingturb_mix ==> /tmp/parcels-2894/libb390fad00b418f4b4ce3ef7e2edef32f_0.so\n",
      "INFO: Temporary output files are stored in /home/jvalenti/MOAD/results/out-RKAZBHDF.\n",
      "INFO: You can use \"parcels_convert_npydir_to_netcdf /home/jvalenti/MOAD/results/out-RKAZBHDF\" to convert these to a NetCDF file during the run.\n",
      "  8%|▊         | 21600.0/259200.0 [00:06<01:31, 2606.43it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m KERNELS \u001b[39m=\u001b[39m kernel_asem(pset,param)\n\u001b[0;32m----> 2\u001b[0m pset\u001b[39m.\u001b[39;49mexecute(KERNELS,\n\u001b[1;32m      3\u001b[0m             runtime\u001b[39m=\u001b[39;49mduration, \n\u001b[1;32m      4\u001b[0m             dt\u001b[39m=\u001b[39;49mdt,\n\u001b[1;32m      5\u001b[0m             output_file\u001b[39m=\u001b[39;49mpset\u001b[39m.\u001b[39;49mParticleFile(name\u001b[39m=\u001b[39;49moutfile, outputdt\u001b[39m=\u001b[39;49mtimedelta(hours\u001b[39m=\u001b[39;49modt)),\n\u001b[1;32m      6\u001b[0m             recovery\u001b[39m=\u001b[39;49m{ErrorCode\u001b[39m.\u001b[39;49mErrorOutOfBounds: DeleteParticle})\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/particleset/baseparticleset.py:507\u001b[0m, in \u001b[0;36mBaseParticleSet.execute\u001b[0;34m(self, pyfunc, pyfunc_inter, endtime, runtime, dt, moviedt, recovery, output_file, movie_background_field, verbose_progress, postIterationCallbacks, callbackdt)\u001b[0m\n\u001b[1;32m    505\u001b[0m     next_callback \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m callbackdt \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msign(dt)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m time \u001b[39m!=\u001b[39m endtime:\n\u001b[0;32m--> 507\u001b[0m     next_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfieldset\u001b[39m.\u001b[39;49mcomputeTimeChunk(time, dt)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/fieldset.py:1106\u001b[0m, in \u001b[0;36mFieldSet.computeTimeChunk\u001b[0;34m(self, time, dt)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m     f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1106\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mcomputeTimeChunk(data, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1108\u001b[0m     f\u001b[39m.\u001b[39mloaded_time_indices \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/field.py:1416\u001b[0m, in \u001b[0;36mField.computeTimeChunk\u001b[0;34m(self, data, tindex)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer_data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m   1415\u001b[0m     buffer_data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mreshape(buffer_data, \u001b[39msum\u001b[39m(((buffer_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, ), buffer_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]), ()))\n\u001b[0;32m-> 1416\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_concatenate(data, buffer_data, tindex)\n\u001b[1;32m   1417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilebuffers[tindex] \u001b[39m=\u001b[39m filebuffer\n\u001b[1;32m   1418\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/field.py:1365\u001b[0m, in \u001b[0;36mField.data_concatenate\u001b[0;34m(self, data, data_to_concat, tindex)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mconcatenate([data_to_concat, data[tindex\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:, :]], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1364\u001b[0m \u001b[39melif\u001b[39;00m tindex \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1365\u001b[0m     data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mconcatenate([data[:tindex, :], data_to_concat], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata_concatenate is used for computeTimeChunk, with tindex in [0, 1]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21600.0/259200.0 [00:19<01:31, 2606.43it/s]"
     ]
    }
   ],
   "source": [
    "KERNELS = kernel_asem(pset,param)\n",
    "pset.execute(KERNELS,\n",
    "            runtime=duration, \n",
    "            dt=dt,\n",
    "            output_file=pset.ParticleFile(name=outfile, outputdt=timedelta(hours=odt)),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c18d5b48c0702a9e14ce9f1a358d9af7e982f07ccbb6648362fbf5930d0b5c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
