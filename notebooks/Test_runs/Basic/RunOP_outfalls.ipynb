{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from parcels import FieldSet, Field, VectorField, ParticleSet, JITParticle, ErrorCode, ParcelsRandom, Variable\n",
    "\n",
    "sys.path.append('/home/jvalenti/MOAD/analysis-jose/Source')\n",
    "\n",
    "from OP_functions import *\n",
    "from OP_Kernels import *\n",
    "\n",
    "local = 0\n",
    "restart=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_deploy(N,MFc,r,dd,dtp):\n",
    "    #r is radius of particle cloud [m]\n",
    "    MFc = float(MFc)\n",
    "    Outfall_deploy = pd.read_csv(N, index_col = [0])\n",
    "    Pol = list(Outfall_deploy.Population)\n",
    "    Lat = Outfall_deploy.Lat\n",
    "    Lon = Outfall_deploy.Lon\n",
    "    Depth = Outfall_deploy.Depth\n",
    "    clat = []\n",
    "    clon = []\n",
    "    cz = []\n",
    "    for i,loc in enumerate(Pol):\n",
    "        for j in range(int(round((loc*250*dtp)/MFc,0))):\n",
    "            clat.append(Lat.iat[i])\n",
    "            clon.append(Lon.iat[i])\n",
    "            try:\n",
    "                cz.append(float(Depth.iat[i]))\n",
    "            except ValueError:\n",
    "                cz.append(2)\n",
    "    N = len(clat)\n",
    "    deg2m = 111000 * np.cos(49 * np.pi / 180)\n",
    "    var = (r / (deg2m * 3))**2\n",
    "    x_offset, y_offset = np.random.multivariate_normal([0, 0], [[var, 0], [0, var]], [n,N]).T\n",
    "    z_offset=np.random.random_sample([N]).T*(dd)\n",
    "    lon = np.zeros([N])\n",
    "    lat = np.zeros([N])\n",
    "    z = np.zeros([N])\n",
    "    for i in range(N):\n",
    "        lon[i]=(clon[i] + x_offset[i])\n",
    "        lat[i]=(clat[i] + y_offset[i])\n",
    "        z[i]=(cz[i] + z_offset[i])\n",
    "    return lon,lat,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(start,length):\n",
    "    timestamps=[]\n",
    "    duration = timedelta(days=length)\n",
    "    for day in range(duration.days):\n",
    "        timestamps.append([start + timedelta(days=day)])\n",
    "    return np.array(timestamps, dtype='datetime64')\n",
    "\n",
    "def find_temp(rootdir):\n",
    "    dirs=[]\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        if os.path.isdir(d):\n",
    "            dirs.append(d)\n",
    "    temp=sorted(dirs, key=lambda x: os.path.getctime(x), reverse=True)[:1][0]\n",
    "    return temp[-12:]\n",
    "\n",
    "def newest(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "paths = path(local)\n",
    "#Load configuration from .yaml\n",
    "param = load_config(['/home/jvalenti/MOAD/analysis-jose/OParcels/yaml/test_outfalls.yaml'])\n",
    "#Definitions\n",
    "start = datetime(param['startdate']['year'], param['startdate']['month'], param['startdate']['day']) #Start date\n",
    "length = param['param']['length'] # Set Time length [days] \n",
    "dt = param['param']['dt'] #toggle between - or + to pick backwards or forwards \n",
    "N0 = param['param']['N'] # number of deploying locations\n",
    "n = param['param']['n'] # 1000   # number of particles per location\n",
    "dmin = param['param']['dmin'] #minimum depth\n",
    "dd = param['param']['dd'] #max depth difference from dmin\n",
    "name = param['file']['name'] #name output file\n",
    "dtp = param['param']['dtp'] #how often particle released in hours\n",
    "odt = param['param']['odt'] #how often data is recorded\n",
    "rrr = param['param']['r'] #radious of particle deployment\n",
    "distr = param['param']['distr']\n",
    "MFc = param['param']['MFc']\n",
    "r = param['param']['r']\n",
    "duration = timedelta(days=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat, z = pandas_deploy(N0,MFc,r,dd,dtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File /ocean/jvalenti/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2023.4.1).\n",
      "         It will be opened with no decoding. Filling values might be wrongly parsed.\n"
     ]
    }
   ],
   "source": [
    "#Set start date time and the name of the output file\n",
    "\n",
    "daterange = [start+timedelta(days=i) for i in range(length)]\n",
    "fn =  name + '_'.join(d.strftime('%Y%m%d')+'_1n' for d in [start, start+duration]) + '.zarr'\n",
    "outfile = os.path.join(paths['out'], fn)\n",
    "####BUILD FIELDS FOR SIMULATION######\n",
    "#Fill in the list of variables that you want to use as fields\n",
    "varlist=['U','V','W']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "dimensions = {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw','time': 'time_counter'}\n",
    "field_set=FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "\n",
    "#Find file names and variable names\n",
    "varlist=['US','VS','WL','Diat','Flag','R','T','S','FS','ssh','Bathy','Kz','totdepth','Vol']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "\n",
    "#Add Stokes Drift fields\n",
    "dimensions = {'lon': 'longitude', 'lat': 'latitude', 'time': 'time'}\n",
    "us = Field.from_netcdf(filenames['US'], variables['US'], dimensions,allow_time_extrapolation=True)\n",
    "vs = Field.from_netcdf(filenames['VS'], variables['VS'], dimensions,allow_time_extrapolation=True)\n",
    "wl = Field.from_netcdf(filenames['WL'], variables['WL'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(us)\n",
    "field_set.add_field(vs)\n",
    "field_set.add_field(wl)\n",
    "field_set.add_vector_field(VectorField(\"stokes\", us, vs, wl))\n",
    "\n",
    "#Add Vertical diffusivity coefficient field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'depthw','time': 'time_counter'}\n",
    "Kz = Field.from_netcdf(filenames['Kz'], variables['Kz'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Kz)\n",
    "\n",
    "#Add fields located at node T\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'deptht','time': 'time_counter'}\n",
    "Diat = Field.from_netcdf(filenames['Diat'], variables['Diat'], dimensions,allow_time_extrapolation=True)\n",
    "Flag = Field.from_netcdf(filenames['Flag'], variables['Flag'], dimensions,allow_time_extrapolation=True)\n",
    "R = Field.from_netcdf(filenames['R'], variables['R'], dimensions,allow_time_extrapolation=True)\n",
    "S = Field.from_netcdf(filenames['S'], variables['S'], dimensions,allow_time_extrapolation=True)\n",
    "T = Field.from_netcdf(filenames['T'], variables['T'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Diat)\n",
    "field_set.add_field(Flag)\n",
    "field_set.add_field(R)\n",
    "field_set.add_field(S)\n",
    "field_set.add_field(T)\n",
    "\n",
    "#Add Bathymetry 2D field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit'}\n",
    "Bth = Field.from_netcdf(filenames['Bathy'], variables['Bathy'], dimensions,allow_time_extrapolation=True)\n",
    "TD = Field.from_netcdf(filenames['totdepth'], variables['totdepth'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Bth)\n",
    "field_set.add_field(TD)\n",
    "\n",
    "#Add Volume 3D field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'depthw'}\n",
    "Vol = Field.from_netcdf(filenames['Vol'], variables['Vol'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(Vol)\n",
    "\n",
    "\n",
    "#Add SSH and Rivers 2D fields\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit','time': 'time_counter'}\n",
    "SSH = Field.from_netcdf(filenames['ssh'], variables['ssh'], dimensions,allow_time_extrapolation=True)\n",
    "field_set.add_field(SSH)\n",
    "Fraser = Field.from_netcdf(filenames['FS'], variables['FS'], dimensions,allow_time_extrapolation=True,timestamps=get_timestamps(start,length))\n",
    "field_set.add_field(Fraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####BUILD Particle typeN######\n",
    "MPParticle = particle_maker(param)\n",
    "\n",
    "if restart==1:\n",
    "    name_temp=find_temp(paths['out'])\n",
    "    os.system(f\"cd {paths['out']} && parcels_convert_npydir_to_netcdf {name_temp}\")\n",
    "    outfile=newest(paths['out'])\n",
    "    pset = ParticleSet.from_particlefile(field_set, MPParticle,outfile)\n",
    "else:\n",
    "    if dtp == 0:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z,time=start+timedelta(hours=odt))\n",
    "    else:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z, repeatdt = timedelta(hours=dtp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled ArrayMPParticleAdvectionBuoyancyStokes_driftturb_mixDisplacementUnbeaching ==> /tmp/parcels-2894/libc815e620efd05b8b103bede4c9f96ab2_0.so\n",
      "INFO: Output files are stored in /home/jvalenti/MOAD/results/Outfalls_202220220101_1n_20220102_1n.zarr.\n",
      " 40%|███▉      | 34200.0/86400.0 [01:33<02:18, 377.73it/s]<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      " 81%|████████▏ | 70200.0/86400.0 [03:51<00:52, 308.62it/s]<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "100%|██████████| 86400.0/86400.0 [05:03<00:00, 284.45it/s]\n"
     ]
    }
   ],
   "source": [
    "KERNELS =  Advection + pset.Kernel(Buoyancy) + pset.Kernel(Stokes_drift) + pset.Kernel(turb_mix) + pset.Kernel(Displacement) + pset.Kernel(Unbeaching)\n",
    "pset.execute(KERNELS,\n",
    "            runtime=duration, \n",
    "            dt=dt,\n",
    "            output_file=pset.ParticleFile(name=outfile, outputdt=timedelta(hours=odt)),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c18d5b48c0702a9e14ce9f1a358d9af7e982f07ccbb6648362fbf5930d0b5c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
