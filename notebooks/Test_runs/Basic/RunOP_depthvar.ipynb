{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from parcels import FieldSet, Field, VectorField, ParticleSet, JITParticle, ParcelsRandom, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(start,length):\n",
    "    timestamps=[]\n",
    "    duration = timedelta(days=length)\n",
    "    for day in range(duration.days):\n",
    "        timestamps.append([start + timedelta(days=day)])\n",
    "    return np.array(timestamps, dtype='datetime64')\n",
    "\n",
    "def find_temp(rootdir):\n",
    "    dirs=[]\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        if os.path.isdir(d):\n",
    "            dirs.append(d)\n",
    "    temp=sorted(dirs, key=lambda x: os.path.getctime(x), reverse=True)[:1][0]\n",
    "    return temp[-12:]\n",
    "\n",
    "def newest(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path(local = 1):\n",
    "    '''Change with your paths'''\n",
    "    if local == 1:\n",
    "        path = {'NEMO': '/Users/jvalenti/MOAD/data/',\n",
    "        'coords': '/Users/jvalenti/MOAD/grid/coordinates_seagrid_SalishSea201702.nc',\n",
    "        'mask': '/Users/jvalenti/MOAD/grid2/mesh_mask202108_TDV.nc',\n",
    "        'out': '/Users/jvalenti/MOAD/results/',\n",
    "        'home': '/Users/jvalenti/MOAD/analysis-jose/notebooks/parcels',\n",
    "        'anim': '/Users/jvalenti/MOAD/animations'}\n",
    "    else:\n",
    "        path = {'NEMO': '/results2/SalishSea/nowcast-green.202111/',\n",
    "        'coords': '/ocean/jvalenti/MOAD/grid/coordinates_seagrid_SalishSea201702.nc',\n",
    "        'coordsWW3': '/ocean/jvalenti/MOAD/grid2/WW3_grid.nc',\n",
    "        'mask': '/ocean/jvalenti/MOAD/grid2/mesh_mask202108_TDV.nc',\n",
    "        'bat': '/ocean/jvalenti/MOAD/grid/bathymetry_202108.nc',\n",
    "        'out': '/home/jvalenti/MOAD/results',\n",
    "        'home': '/home/jvalenti/MOAD/analysis-jose/notebooks/parcels',\n",
    "        'anim': '/home/jvalenti/MOAD/animations'}\n",
    "    return path\n",
    "\n",
    "def load_config(config_yaml):\n",
    "   with open(config_yaml[0]) as f:\n",
    "       config = yaml.safe_load(f)\n",
    "   return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "local=0\n",
    "paths = path(local)\n",
    "#Load configuration from .yaml\n",
    "param = load_config(['/home/jvalenti/MOAD/analysis-jose/OParcels/yaml/test.yaml'])\n",
    "#Definitions\n",
    "start = datetime(param['startdate']['year'], param['startdate']['month'], param['startdate']['day']) #Start date\n",
    "length = param['param']['length'] # Set Time length [days] \n",
    "dt = param['param']['dt'] #toggle between - or + to pick backwards or forwards \n",
    "N0 = param['param']['N'] # number of deploying locations\n",
    "n = param['param']['n'] # 1000   # number of particles per location\n",
    "dmin = param['param']['dmin'] #minimum depth\n",
    "dd = param['param']['dd'] #max depth difference from dmin\n",
    "name = param['file']['name'] #name output file\n",
    "dtp = param['param']['dtp'] #how often particle released in hours\n",
    "odt = param['param']['odt'] #how often data is recorded\n",
    "rrr = param['param']['r'] #radious of particle deployment\n",
    "distr = param['param']['distr']\n",
    "MFc = param['param']['MFc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_deploy(N,n,dmin,dd,r = 1000):\n",
    "    #r is radius of particle cloud [m]\n",
    "    deg2m = 111000 * np.cos(50 * np.pi / 180)\n",
    "    var = (r / (deg2m * 3))**2\n",
    "    x_offset, y_offset = np.random.multivariate_normal([0, 0], [[var, 0], [0, var]], [n,N]).T\n",
    "    if isinstance(dmin,int):\n",
    "        zvals1 = dmin + np.random.random_sample([n,N]).T*(dd)\n",
    "    else:\n",
    "        zvals = []\n",
    "        zvals1 = []\n",
    "        for dept in dmin:\n",
    "            zvals.append(dept + np.random.random_sample([n]).T*(dd))\n",
    "        for i in range(len(zvals)):   \n",
    "            zvals1=np.concatenate((zvals1[:],zvals[i]))\n",
    "    return x_offset, y_offset, zvals1\n",
    "\n",
    "\n",
    "def make_prefix(date, path, res='h'):\n",
    "    \"\"\"Construct path prefix for local SalishSeaCast results given date object and paths dict\n",
    "    e.g., /results2/SalishSea/nowcast-green.201905/daymonthyear/SalishSea_1h_yyyymmdd_yyyymmdd\n",
    "    \"\"\"\n",
    "\n",
    "    datestr = '_'.join(np.repeat(date.strftime('%Y%m%d'), 2))\n",
    "    folder = date.strftime(\"%d%b%y\").lower()\n",
    "    prefix = os.path.join(path, f'{folder}/SalishSea_1{res}_{datestr}')\n",
    "    \n",
    "    return prefix\n",
    "\n",
    "def get_WW3_path(date):\n",
    "    \"\"\"Construct WW3 results path given the date\n",
    "    e.g., /opp/wwatch3/nowcast/SoG_ww3_fields_YYYYMMDD_YYYYMMDD.nc\n",
    "    :arg date: date of WW3 record\n",
    "    :type date: :py:class:`datetime.datetime`\n",
    "    :returns: WW3 path\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # Make WW3 path\n",
    "    path = '/opp/wwatch3/hindcast'\n",
    "    path2 = '/opp/wwatch3/nowcast'\n",
    "    datestr = [date.strftime(fmt) for fmt in ('%d%b%y', '%Y%m%d_%Y%m%d')]\n",
    "    path = os.path.join(path, datestr[0].lower(), f'SoG_ww3_fields_{datestr[1]}.nc')\n",
    "    if not os.path.exists(path):\n",
    "        path = os.path.join(path2, datestr[0].lower(), f'SoG_ww3_fields_{datestr[1]}.nc')\n",
    "        if not os.path.exists(path):    \n",
    "            raise ValueError(f\"No WW3 record found for the specified date {date.strftime('%Y-%b-%d')}\")\n",
    "\n",
    "    return path\n",
    "\n",
    "def get_Fraser_path(date):\n",
    "    \"\"\"Construct Fraser river outflow path given the date\n",
    "    e.g., /results/forcing/rivers/#R201702DFraCElse_yYYYYmMMdDD.nc\n",
    "    :arg date: date of fraser outflow record \n",
    "    :type date: :py:class:`datetime.datetime`\n",
    "    :returns: nc path\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # Make Fraser path\n",
    "    path = '/results/forcing/rivers/'\n",
    "    datestr = [date.strftime(fmt) for fmt in ('%d%b%y', 'y%Ym%md%d')]\n",
    "    path = os.path.join(path, f'R201702DFraCElse_{datestr[1]}.nc')\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(f\"No file found for the specified date {date.strftime('%Y-%b-%d')}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set deploy coordinates following yaml   \n",
    "\n",
    "clat = param['param']['lats']\n",
    "clon = param['param']['lons']\n",
    "#clon, clat = [float(outf_lon)],[float(outf_lat)] \n",
    "duration = timedelta(days=length)\n",
    "#Set deploy locations\n",
    "if distr == 'hmg':\n",
    "    clat,clon = p_unidist(N0,N0)\n",
    "    N = len(clat)\n",
    "elif distr == 'trst':\n",
    "    clat,clon = transect_deploy(clat,clon,N0)\n",
    "    N = N0\n",
    "elif distr == 'std':\n",
    "    N = len(param['param']['lats'])\n",
    "elif distr == 'pd':\n",
    "    clat, clon, N = pandas_deploy(N0,MFc,int(dtp))\n",
    "    n = 1\n",
    "\n",
    "x_offset, y_offset, z = p_deploy(N,n,dmin,dd,rrr)\n",
    "\n",
    "lon = np.zeros([N,n])\n",
    "lat = np.zeros([N,n])\n",
    "for i in range(N):\n",
    "    lon[i,:]=(clon[i] + x_offset[i,:])\n",
    "    lat[i,:]=(clat[i] + y_offset[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set start date time and the name of the output file\n",
    "\n",
    "daterange = [start+timedelta(days=i) for i in range(length)]\n",
    "fn =  name + '_'.join(d.strftime('%Y%m%d')+'_1n' for d in [start, start+duration]) + '.nc'\n",
    "outfile = os.path.join(paths['out'], fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_set(start,length,varlist=['U','V','W'],local=0):\n",
    "    '''filename,variables,dimensions = filename_set(start,duration,varlist=['U','V','W'],local=1)\n",
    "    Modify function to include more default variables\n",
    "    define start as: e.g, datetime(2018, 1, 17)\n",
    "    length= number of days'''\n",
    "    \n",
    "    duration = timedelta(days=length)\n",
    "    #Build filenames\n",
    "    paths = path(local)\n",
    "    Rlist,Tlist,Ulist, Vlist, Wlist = [], [], [], [], []\n",
    "    Waveslist = []\n",
    "    Flist = []\n",
    "    Biolist,MZlist = [],[]\n",
    "   \n",
    "    for day in range(duration.days):\n",
    "        path_NEMO = make_prefix(start + timedelta(days=day), paths['NEMO'])\n",
    "        path_NEMO_d = make_prefix(start + timedelta(days=day), paths['NEMO'],res='d')\n",
    "        Ulist.append(path_NEMO + '_grid_U.nc')\n",
    "        Vlist.append(path_NEMO + '_grid_V.nc')\n",
    "        Wlist.append(path_NEMO + '_grid_W.nc')\n",
    "        Tlist.append(path_NEMO + '_grid_T.nc')\n",
    "        Biolist.append(path_NEMO_d + '_prod_T.nc')\n",
    "        Waveslist.append(get_WW3_path(start + timedelta(days=day)))\n",
    "        Flist.append(get_Fraser_path(start + timedelta(days=day)))\n",
    "        \n",
    "\n",
    "    # Load NEMO forcing \n",
    "    filenames = {\n",
    "        'U': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Ulist, 'data': Ulist},\n",
    "        'V': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Ulist, 'data': Vlist},\n",
    "        'W': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Wlist[0], 'data': Wlist},\n",
    "        'Kz': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Wlist[0], 'data': Wlist},\n",
    "        'T': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Tlist[0], 'data': Tlist},\n",
    "        'S': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Tlist[0], 'data': Tlist},\n",
    "        'ssh': {'lon': paths['coords'], 'lat': paths['coords'], 'data': Tlist},\n",
    "        'R': {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Tlist[0], 'data': Tlist},\n",
    "        'Bathy' : {'lon': paths['coords'], 'lat': paths['coords'], 'data': paths['bat']},\n",
    "        'gdepth' : {'lon': paths['coords'], 'lat': paths['coords'],'depth': Ulist, 'data': paths['mask']},\n",
    "        'totdepth' : {'lon': paths['coords'], 'lat': paths['coords'], 'data': paths['mask']},\n",
    "        'US' : {'lon': paths['coordsWW3'], 'lat': paths['coordsWW3'], 'data': Waveslist},\n",
    "        'VS' : {'lon': paths['coordsWW3'], 'lat': paths['coordsWW3'], 'data': Waveslist},\n",
    "        'WL' : {'lon': paths['coordsWW3'], 'lat': paths['coordsWW3'], 'data': Waveslist},\n",
    "        'FS' :  {'lon': paths['coords'], 'lat': paths['coords'],'data': Flist},\n",
    "        'Diat' : {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Tlist[0], 'data': Biolist},\n",
    "        'Flag' : {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Tlist[0], 'data': Biolist},\n",
    "        'Vol' : {'lon': paths['coords'], 'lat': paths['coords'], 'depth': Wlist[0],'data': paths['mask']}\n",
    "    }\n",
    "    variables = {'U': 'vozocrtx', 'V': 'vomecrty','W': 'vovecrtz','gdepth':\"depthu\",'T':'votemper','S':'vosaline','R':'sigma_theta',\n",
    "        'US':'uuss','VS':'vuss','WL':'lm','Bathy':'Bathymetry','FS':'rorunoff','Kz':'vert_eddy_diff',\n",
    "        'MZ':'microzooplankton','Diat':'PPDIATNO3','Flag':'PPPHYNO3','ssh':'sossheig','totdepth':'totaldepth','Vol':'volume'}\n",
    "        \n",
    "    file2,var2 = {},{}\n",
    "    for var in varlist:\n",
    "        file2[var]=filenames[var]\n",
    "        var2[var]=variables[var]\n",
    "    return file2,var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist=['U','V','gdepth']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "\n",
    "dimensions = {\n",
    "    \"U\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"depth\": \"not_yet_set\", \"time\": \"time_counter\"},\n",
    "    \"V\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"depth\": \"not_yet_set\", \"time\": \"time_counter\"},\n",
    "    \"depth_u\": {\"lon\": \"glamf\", \"lat\": \"gphif\", \"depth\": \"not_yet_set\", \"time\": \"time_counter\"},\n",
    "}\n",
    "variables = {\n",
    "    \"U\": \"vozocrtx\",\n",
    "    \"V\": \"vomecrty\",\n",
    "    \"gdepth\": \"depthu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Vertically adaptive meshes not implemented for from_netcdf()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fieldset \u001b[38;5;241m=\u001b[39m \u001b[43mFieldSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_netcdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_time_extrapolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m fieldset\u001b[38;5;241m.\u001b[39mU\u001b[38;5;241m.\u001b[39mset_depth_from_field(fieldset\u001b[38;5;241m.\u001b[39mdepth_u)\n\u001b[1;32m      5\u001b[0m fieldset\u001b[38;5;241m.\u001b[39mV\u001b[38;5;241m.\u001b[39mset_depth_from_field(fieldset\u001b[38;5;241m.\u001b[39mdepth_u)\n",
      "File \u001b[0;32m~/conda_envs/Parcels_24/lib/python3.11/site-packages/parcels/fieldset.py:454\u001b[0m, in \u001b[0;36mFieldSet.from_netcdf\u001b[0;34m(cls, filenames, variables, dimensions, indices, fieldtype, mesh, timestamps, allow_time_extrapolation, time_periodic, deferred_load, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                     dFiles \u001b[38;5;241m=\u001b[39m fields[procvar]\u001b[38;5;241m.\u001b[39mdataFiles\n\u001b[1;32m    453\u001b[0m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     fields[var] \u001b[38;5;241m=\u001b[39m \u001b[43mField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mallow_time_extrapolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_time_extrapolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtime_periodic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_periodic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeferred_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeferred_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfieldtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfieldtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvarchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataFiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m u \u001b[38;5;241m=\u001b[39m fields\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    460\u001b[0m v \u001b[38;5;241m=\u001b[39m fields\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda_envs/Parcels_24/lib/python3.11/site-packages/parcels/field.py:407\u001b[0m, in \u001b[0;36mField.from_netcdf\u001b[0;34m(cls, filenames, variable, dimensions, indices, grid, mesh, timestamps, allow_time_extrapolation, time_periodic, deferred_load, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     depth_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_dim_filenames(filenames, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filenames, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(depth_filename) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVertically adaptive meshes not implemented for from_netcdf()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    408\u001b[0m     depth_filename \u001b[38;5;241m=\u001b[39m depth_filename[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    410\u001b[0m netcdf_engine \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetcdf_engine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetcdf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Vertically adaptive meshes not implemented for from_netcdf()"
     ]
    }
   ],
   "source": [
    "fieldset = FieldSet.from_netcdf(\n",
    "    filenames, variables, dimensions, mesh=\"flat\", allow_time_extrapolation=True\n",
    ")\n",
    "fieldset.U.set_depth_from_field(fieldset.depth_u)\n",
    "fieldset.V.set_depth_from_field(fieldset.depth_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####BUILD FIELDS FOR SIMULATION######\n",
    "#Fill in the list of variables that you want to use as fields\n",
    "varlist=['U','V','W']\n",
    "filenames,variables=filename_set(start,length,varlist,local)\n",
    "dimensions = {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw','time': 'time_counter'}\n",
    "field_set=FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True)\n",
    "\n",
    "####BUILD Particle typeN######\n",
    "MPParticle = particle_maker(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart==1:\n",
    "    name_temp=find_temp(paths['out'])\n",
    "    os.system(f\"cd {paths['out']} && parcels_convert_npydir_to_netcdf {name_temp}\")\n",
    "    outfile=newest(paths['out'])\n",
    "    pset = ParticleSet.from_particlefile(field_set, MPParticle,outfile)\n",
    "else:\n",
    "    if dtp == 0:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z,time=start+timedelta(hours=odt))\n",
    "    else:\n",
    "        pset = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z, repeatdt = timedelta(hours=dtp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_asem(pset,config):\n",
    "    KER = AdvectionRK4_3D \n",
    "    if 'Buoyancy' in config['kernel']:\n",
    "        KER += pset.Kernel(Buoyancy)\n",
    "    if 'Stokes_drift' in config['kernel']:\n",
    "        KER += pset.Kernel(Stokes_drift)\n",
    "    if 'Beaching' in config['kernel']:\n",
    "        KER += pset.Kernel(Beaching)\n",
    "        KER += pset.Kernel(Unbeaching)\n",
    "    if 'Turb_mix' in config['kernel']:\n",
    "        KER += pset.Kernel(turb_mix)\n",
    "    if 'Biofilm' in config['kernel']:\n",
    "        KER += pset.Kernel(Biofilm)\n",
    "    if 'Stokes_driftRK4_3D' in config['kernel']:\n",
    "        KER += pset.Kernel(Stokes_driftRK4_3D)\n",
    "    \n",
    "    return KER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Compiled ArrayMPParticleAdvectionRK4_3DStokes_driftBeachingUnbeachingturb_mix ==> /tmp/parcels-2894/libb390fad00b418f4b4ce3ef7e2edef32f_0.so\n",
      "INFO: Temporary output files are stored in /home/jvalenti/MOAD/results/out-RKAZBHDF.\n",
      "INFO: You can use \"parcels_convert_npydir_to_netcdf /home/jvalenti/MOAD/results/out-RKAZBHDF\" to convert these to a NetCDF file during the run.\n",
      "  8%|▊         | 21600.0/259200.0 [00:06<01:31, 2606.43it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m KERNELS \u001b[39m=\u001b[39m kernel_asem(pset,param)\n\u001b[0;32m----> 2\u001b[0m pset\u001b[39m.\u001b[39;49mexecute(KERNELS,\n\u001b[1;32m      3\u001b[0m             runtime\u001b[39m=\u001b[39;49mduration, \n\u001b[1;32m      4\u001b[0m             dt\u001b[39m=\u001b[39;49mdt,\n\u001b[1;32m      5\u001b[0m             output_file\u001b[39m=\u001b[39;49mpset\u001b[39m.\u001b[39;49mParticleFile(name\u001b[39m=\u001b[39;49moutfile, outputdt\u001b[39m=\u001b[39;49mtimedelta(hours\u001b[39m=\u001b[39;49modt)),\n\u001b[1;32m      6\u001b[0m             recovery\u001b[39m=\u001b[39;49m{ErrorCode\u001b[39m.\u001b[39;49mErrorOutOfBounds: DeleteParticle})\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/particleset/baseparticleset.py:507\u001b[0m, in \u001b[0;36mBaseParticleSet.execute\u001b[0;34m(self, pyfunc, pyfunc_inter, endtime, runtime, dt, moviedt, recovery, output_file, movie_background_field, verbose_progress, postIterationCallbacks, callbackdt)\u001b[0m\n\u001b[1;32m    505\u001b[0m     next_callback \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m callbackdt \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msign(dt)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m time \u001b[39m!=\u001b[39m endtime:\n\u001b[0;32m--> 507\u001b[0m     next_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfieldset\u001b[39m.\u001b[39;49mcomputeTimeChunk(time, dt)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/fieldset.py:1106\u001b[0m, in \u001b[0;36mFieldSet.computeTimeChunk\u001b[0;34m(self, time, dt)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m     f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mfilebuffers[\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1106\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mcomputeTimeChunk(data, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1108\u001b[0m     f\u001b[39m.\u001b[39mloaded_time_indices \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/field.py:1416\u001b[0m, in \u001b[0;36mField.computeTimeChunk\u001b[0;34m(self, data, tindex)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer_data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m   1415\u001b[0m     buffer_data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mreshape(buffer_data, \u001b[39msum\u001b[39m(((buffer_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, ), buffer_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]), ()))\n\u001b[0;32m-> 1416\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_concatenate(data, buffer_data, tindex)\n\u001b[1;32m   1417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilebuffers[tindex] \u001b[39m=\u001b[39m filebuffer\n\u001b[1;32m   1418\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/conda_envs/general/lib/python3.10/site-packages/parcels/field.py:1365\u001b[0m, in \u001b[0;36mField.data_concatenate\u001b[0;34m(self, data, data_to_concat, tindex)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mconcatenate([data_to_concat, data[tindex\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:, :]], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1364\u001b[0m \u001b[39melif\u001b[39;00m tindex \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1365\u001b[0m     data \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mconcatenate([data[:tindex, :], data_to_concat], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata_concatenate is used for computeTimeChunk, with tindex in [0, 1]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21600.0/259200.0 [00:19<01:31, 2606.43it/s]"
     ]
    }
   ],
   "source": [
    "KERNELS = kernel_asem(pset,param)\n",
    "pset.execute(KERNELS,\n",
    "            runtime=duration, \n",
    "            dt=dt,\n",
    "            output_file=pset.ParticleFile(name=outfile, outputdt=timedelta(hours=odt)),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c18d5b48c0702a9e14ce9f1a358d9af7e982f07ccbb6648362fbf5930d0b5c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
